#+hugo_base_dir: ../
* DONE AWS and the Enshittification of Educational Workflows    :teaching:AI:
CLOSED: [2025-10-20 Mon 14:25]
:PROPERTIES:
:EXPORT_FILE_NAME: aws-and-the-enshittification-of-educational-workflows
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :summary "Canvas is down."
:END:
I am currently sitting in my office on campus. It is 1:57PM EST. I had planned a number of things to do today, most of which involved my teaching work: grading, lesson planning for tomorrow, following up with some students, etc. None of that is happening because, as an educational society, we have given up everything to LMS's. And AWS has them down.

As a lot of folks are experiencing right now, Amazon Web Services, the infrastructure on which a good chunk of the modern web runs, is not working correctly. [[https://www.nytimes.com/2025/10/20/business/aws-down-internet-outage.html][The reasons for the outage are unclear]], and many websites are back online. Canvas, however--the learning management system that universities around the world have partnered with and require teachers to use for essentially all course-related activities--remains down.

It will be back up soon. My gripe is more with the fact that this is a perfect example of why [[/posts/2024-10-28-content-management-pedagogy/][content management pedagogy]] is such an odious phenomenon. The centralized role of learning management systems like Canvas leads to the following issues, for me, today:

- I can't access assignments to grade, because students submit them to Canvas.
- I can't access or update course materials. They are all required to be on Canvas.
- I can't communicate with my students as a whole. Believe it or not, I am fairly certain that there is no actual way for me to message my classes except through Canvas. I could search for their names, one by one, in Outlook (another technology I don't like for another discussion), but guess where my course rosters are?

Cory Doctorow's concept of [[https://www.wired.com/story/tiktok-platforms-cory-doctorow/][enshittification]] refers to a predictable pattern where technologies create value for users, create value for businesses, create value for themselves, and then die because their platform no longer provides any use, enjoyment, or value for anyone. Monopolization, he explains, is both a feature and an outcome involved in this process. Once a tool becomes the only game in town, it's free to dominate the contexts in which it's used.

Canvas isn't a monopoly, but at a broader scale LMS's have monopolized education. I often ask my students: do you all even like learning management systems like LMS's? Not one student has ever indicated anything positive about them.

Yet, we still have to use them, because schools (not just universities) in the US have ceded educational ground to them, feeding them money and [[https://dl.acm.org/doi/10.1145/3613904.3642914][almost certainly student data]], becoming more and more reliant on enormous tech companies in ways that are honestly divergent from the goals of education. I suppose it's too much to ask at this current moment when every educational institution is scrambling to make deals with artificial intelligence companies that are, themselves, experiencing huge economic disruption and a growing sense that [[https://www.wsj.com/tech/ai/ai-bubble-building-spree-55ee6128][the bubble will soon burst]] but... I wish we could take the long-term view that [[/posts/2024-09-25-cannon-shot-epistemology/][digital technologies change over time]], often for the worse, into account. If not, we risk enshittifying education itself.
* DONE Hypernormalized AI
CLOSED: [2025-10-24 Fri 15:51]
:PROPERTIES:
:EXPORT_FILE_NAME: hypernormalized-ai
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :summary "The push for normalizing AI is getting frantic."
:END:

[[https://www.youtube.com/watch?v=AUiqaFIONPQ][/HyperNormalisation/]], a BBC documentary by the film-maker Adam Curtis, is interesting to think about amidst all this AI talk. So much AI talk. An embarrassment, frankly, of AI talk. Or, at least, an embarassment of AI talk that is, based on the arguments and ideas that tend to dominate the conversation, embarassing.

I won't spend too much time outlining the film. It's quite weird, and the connections to AI are a little oblique. Essentially, Curtis takes up Alexei Yurchak's theory that toward the end of the Soviet Union, a phenomenon of "hypernormalization" occured where everyone was compelled to uncritically, rapidly, and consistently accept new social fictions. The argument of the film is that since the ~1970s, technologists have given up dealing with the problems of the real world and instead invented the virtual world (mass media and the Internet) to benefit the neoliberal order.

Contrary to what you might think, the connection /isn't/ that the powerful are going to trap us inside an LLM, or that AI is going to enslave humanity, or any of the other [[https://doctorow.medium.com/the-real-ai-fight-1ce751886457][delusive misdirections]] that Silicon Valley doomers are, honestly or cynically, pumping out. The connection is speed, a condition where society is /purposefully/ denied deliberative time on technologies that companies are heavily invested in gaining widespread use. Much of the world around us trying to hypernormalize AI.

One way that this is being done in the social sphere is through AI increasingly being adapted "subtly" as topics within media. While it didn't gain much success (for unrelated, quality-based reasons), /M3GAN 2.0/ notably opens with very "random" connection to Iranian villains before presenting a narrative about an AI regulator coming to learn that the problem isn't with AI technology; it's with the humans who employ it. Similarly, the most recent season of /Only Murders in the Building/ represents LSTR as an AI robot that ends up a sort of sub-hero within the plot. Like in /M3GAN/, LSTR has a monologue explaining that it's humans that make each other's lives unhappy, not technology. AI doesn't hurt people, people hurt people. Or consider that AMC is showing an ad for Google Gemini that is designed to look like a movie trailer, and comes right between two real ones.

Just look at the technology you're using. How many tools do you use that now have some GenAI feature popping up at you? I would hazard to guess that whether you like these tools or not, you weren't consulted. They were, simply, there one day, normalized as part of the interface. Or think about [[https://www.apple.com/apple-intelligence/][Apple Intelligence]]. Or how ads for computers now emphasize being [[https://www.lenovo.com/us/en/copilot-plus-pc/]["Copilot+."]] Or how AI is already at use [[https://www.armyupress.army.mil/Journals/Military-Review/Online-Exclusive/2024-OLE/AI-Combat-Multiplier/][in the military]]. Or how it's used to [[https://en.wikipedia.org/wiki/AI-assisted_targeting_in_the_Gaza_Strip][target Palestinians]] for state execution. Or...

This is hypernormalization, accelerating because tech companies depend on these tools being socially normalized. It is increasingly reported that, as [[https://www.wsj.com/articles/ai-economics-are-brutal-dema][Rosenbush writes in the WSJ]], "the economics of artificial intelligence have turned sharply against them, at least for now, and for reasons that weren't widely anticipated." [[https://www.wsj.com/articles/oracle-co-ceos-defend-massive-data-center-expansion-plan-to-offer-ai-ecosystem-af644dee][Urgent concerns of AI tech companies]] include major capital spending, high valuations, high debt, and the circular nature of tech with "AI firms pouring money into other firms." Investors are getting concerned by the increasing reality that AI is a bubble showing signs of bursting. As Rosenbush writes, "a handful of players would probably escape a sector collapse and go on to change the world, just like the dot-com survivors did. But even the most likely eventual winners in AI are losing billions of dollars right now.‚Äù While [[https://www.wsj.com/tech/ai/west-texas-data-center-nvidia-e38a4678][data centers are being positioned as the next big push]] for AI corporate value (with all the [[https://mitpress.mit.edu/9780262529969/a-prehistory-of-the-cloud/][geopolitical and environmental concerns]] that accompany them), these companies are getting itchy, and their attempts to normalize the tech for the purpose of "productivity" will get faster and more frantic.

What's sad, to close this brief post, is that intellectual work and scholarship in many fields--including my own--is participating in this hypernormalization. Critiques of universities' exceedingly close relationships notwithstanding (those are table stakes), the amount of scholarship being put out that seems to accept the premise that GenAI is here to stay, right at the moment that its bubble seems most likely to pop is--to return to the beginning--embarassing. Speaking for my field, rhetoric, it's true that since Plato's time we've been a fundamentally reactive field that acquiesces to technological shifts. Perhaps it's due to our historical status as a "meta-discipline" that, honestly, gives us little to claim as our own. Or, more cynically, maybe it's the academic careers being born that, like tech companies, rely on their normalization...

Either way, it feels like much scholarship has gone beyond Sloterdijk's enlightened false consciousness, the unhappy state where we know all the problems but have cynically given up finding solutions. Rather, we seem caught in an unstated but observable desire to /accelerate/ the uptake of a new medium, not to study it or, much less, to change it. Same as it ever was, I suppose, but it would be nice to learn.
