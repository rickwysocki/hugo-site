<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">








    






<link rel="icon" type="image/ico" href="//localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="//localhost:1313/android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="//localhost:1313/apple-touch-icon.png">

<meta name="description" content=""/>



<title>
    
    Cannon Shot Epistemology | output error
    
</title>

<link rel="canonical" href="//localhost:1313/posts/cannon-shot-epistemology/"/>

<meta property="og:url" content="//localhost:1313/posts/cannon-shot-epistemology/">
  <meta property="og:site_name" content="output error">
  <meta property="og:title" content="Cannon Shot Epistemology">
  <meta property="og:description" content="Maybe we shouldn’t base our thinking on what AI is now. Maybe we should think harder about what it could become.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-25T00:00:00-04:00">
    <meta property="article:modified_time" content="2024-09-25T00:00:00-04:00">
    <meta property="article:tag" content="Theory">
    <meta property="article:tag" content="AI">












<link rel="stylesheet" href="/assets/combined.min.403fefeb55ad9a9e72af41abf4538457b584695b1fdcef661626b87d8b07dec9.css" media="all">











    




</head>







<body class="auto">

  <div class="content">
    <header>
      

<div class="header">

    

    <h1 class="header-title">
        <a href="//localhost:1313/">output error</a>
    </h1>

    <div class="header-menu">
        

        
        

        <p
            class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        

        <p
            class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        

        <p
            class="small ">
            <a href="/tags" >
                /tags
            </a>
        </p>
        
        
    </div>

    

</div>

    </header>

    <main class="main">
      




<div class="breadcrumbs"><a href="/">Home</a><span class="breadcrumbs-separator">/</span><a href="/posts/">Posts</a><span class="breadcrumbs-separator">/</span>
        <a href="/posts/cannon-shot-epistemology/">Cannon Shot Epistemology</a></div>


<div >
  <article>
    <header class="single-intro-container">
        
        <h1 class="single-title">Cannon Shot Epistemology</h1>
        <p class="single-summary">Maybe we shouldn&rsquo;t base our thinking on what AI is now. Maybe we should think harder about what it could become.</p>
        
        <div class="single-subsummary">
          
          <div>
            
            <p class="author">[Rick Wysocki] </p>
            
            <p class="single-date">
              <time datetime="2024-09-25T00:00:00-04:00">September 25, 2024</time>
            </p>
          </div>
        </div>
        
    </header>
    
    <div class="single-content">
      <p>Shockingly (to myself), I&rsquo;ve fallen into a research project over the
past few months. In the process, I learned a small fact that I just
can&rsquo;t stop thinking about, and it connects to my dissatisfaction about
the takes I&rsquo;ve seen response to AI from educators. The connection is
oblique, but bear with me.</p>
<p>In earlier versions of the Western interstate system, the seas were
managed by the freedom of the seas doctrine, which tried to balance
nations&rsquo; coastal sovereignty against the use of the oceans for trade. In
the 18th century, this doctrine was upheld by a principle referred to as
the cannon-shot rule, where the territory of a state could be said to
extend only one coastal cannon-shot into its adjacent oceans. This crude
technique provided an available technical measurement for &ldquo;slicing&rdquo;
Earth within the freedom-of-the-seas doctrine that was held at the time.</p>
<p>A fascinatingly human detail is that it was apparently not considered
that cannons would eventually be made to shoot further, until they
could. As Tirza Meyer writes:</p>
<blockquote>
<p>As technology evolved &ndash;including even the simple fact that cannon
range increased &ndash; the necessity to establish exactly how far
territorial waters reached had become a pressing legal issue by the
early twentieth century. (43)</p></blockquote>
<p>The cannon-shot rule as an interstate agreement, then, required a
fictive reliability, a sense that:</p>
<ol>
<li>a technology would <em>not</em> progress,</li>
</ol>
<p>and</p>
<ol>
<li>that it could be thoroughly and reliably understood.</li>
</ol>
<p>The story relates to Paul Virilio&rsquo;s theory of <em>the accident.</em> The
accident here refers to the inherent unreliability of technological
progress, the reality that technical development can only be managed up
to a point. As Virilio explained in a
<a href="https://v2.nl/articles/surfing-the-accident">1998 interview</a>:</p>
<blockquote>
<p>You cannot separate the accident from reality. The accident is merely
the other face of substance, and Aristotle defined it already as such.
According to Aristotle, reality is a mixture of &ldquo;substans&rdquo; (i.e. what
is well established, from the Latin &ldquo;substare&rdquo;), and of &ldquo;accidens&rdquo;
(what &ldquo;falls into,&rdquo; from &ldquo;accidere&rdquo;). He characterized &ldquo;substans&rdquo; as
absolute and necessary, and &ldquo;accidens&rdquo; as relative and fortuitous.
Consequently, reality is made up of these two dimensions. As soon as
something is well established (a substance), it is necessarily
accompanied by something unreliable, which can trigger off forces
difficult to contain at any moment. Technology can only progress in a
struggle against the accident.</p></blockquote>
<p>For Virilio, these technological accidents used to be more localized.
But, given the interconnected nature of the modern world-system,
technical accidents have become ever more &ldquo;integral,&rdquo; that is,
globally-implicating.</p>
<figure><img src="/posts/cannon-shot-epistemology/virilio.jpg"
    alt="Paul Virilio">
</figure>

<p>What&rsquo;s interesting to me, here, is that the technical accident relies on
a limited knowledge about the future and an inability to predict
technological development. Moreover, the questions we <em>do</em> ask about
technological development
<a href="https://www.ben-evans.com/benedictevans/2017/01/11/wrongquestions">are
often the wrong ones</a>, because we can&rsquo;t extrapolate present conditions
into the future. This was clearly the case for the establishment of the
cannon-shot rule. It offered an available measurement but couldn&rsquo;t
predict its integral accident, the unknown technical progress of the
cannon and its effects on legal interstate relations.</p>
<p>This inability to reckon with and ultimately face <em>unknowingness</em> is, in
my opinion, <em>the central</em> <em>problem</em> of current conversations about AI in
higher education. We simply don&rsquo;t have a roadmap here. Some would like
to travel the well-worn territory of how we responded to earlier
technologies, which is endearingly human but wrong. Others want to not
engage with the issue at all.</p>
<p>But more and more I feel like higher ed isn&rsquo;t asking the right
questions, either about generative AI&rsquo;s substance or its possible
accidents. Environmental concerns are certainly an obvious integral
accident, not only of AI but of networked technologies in general. But
what other accidents can we try (to the extent that can try) to imagine?</p>
<p>It&rsquo;s worth noting that Virilio was explicitly considering the
possibility of AI in thinking through these questions. In the same
interview, he says:</p>
<blockquote>
<p>Well, it is true that the fifth generation computers will not only be
able to learn but also to bring forth other computers. What bothers me
most in this idea of self-learning computers is the closed circuit
character of these systems. The world of computing generally is
plagued by this closed loop problem, which is what makes it so
dangerous in the hands of a totalitarian system. In order to avoid
this &ldquo;Gleichschaltung,&rdquo; as the Nazis called it, it is necessary to
structure new computer systems as open systems.</p></blockquote>
<p>Even this comment, from 1998, suggests something that academic
institutions are, predictably, ignoring: the proprietary and closed
nature of the AI systems that we will seemingly very likely be building,
in one way or another, into the foundations of learning in years to
come. This may not pose a clear problem now, but there is always an
accident around the corner.</p>
<p>Are we treating AI as the evolving technology that it is? Or are we
spending our time building &ldquo;cannon shot rules&rdquo; around ultimately
unstable and undetermined assumptions? I&rsquo;m starting to think about
&ldquo;cannon-shot epistemology&rdquo; to name the ways we build knowledge and
infrastructure on an human but incorrect assumption of stable
technology.</p>
<p>So, what does this look like? Here&rsquo;s an example. When folks in higher ed
conversations <em>do</em> try to imagine the future of AI, they demonstrate
cannon shot epistemology in observably specific ways.</p>
<p>A
<a href="https://www.insidehighered.com/opinion/blogs/online-trending-now/2024/09/25/near-future-vision-ai-higher-ed">recent
opinion piece by Ray Schroeder</a> published by <em>Inside Higher Ed</em>
demonstrates them. It starts well enough by acknowledging that AI is
outpacing higher education but that we still need to find ways to
imagine its future in order to respond to it effectively. Good. I agree.
This, so far, is exactly right.</p>
<p>But where does Schroeder&rsquo;s imagination take him?</p>
<blockquote>
<p>I see us replacing midlevel administrators with intelligent agents
that can efficiently and effectively make decisions that are
thoroughly documented and adaptive to changing goals and outcomes.</p></blockquote>
<p>One paragraph later:</p>
<blockquote>
<p>Startling as it may seem to some, I can see these advanced models,
such as those with Ph.D. reasoning, filling adjunct faculty posts
while overseen by human professors. The long-running OpenAI-funded
Khanmigo project demonstrates that key teaching, tutoring and
personalization skills can be delivered by generative AI.</p></blockquote>
<p>Putting aside the obvious and offensive groundwork being laid <strong>against</strong>
non-tenure faculty and administrative staff, <em>why is it that everyone&rsquo;s
imagination seems to exhaust itself just before AI could possibly affect
<strong>them?!</strong></em> Elsewhere in this exact same article, Schroeder discusses how
&ldquo;Multiple societies have been formed and fascinating communities have
been built by intelligent agents.&rdquo; He&rsquo;s talking about Minecraft, which
is embarrassing, but the point is this: he can imagine AI creating
societies, but he can&rsquo;t imagine AI developing one half-inch past firing
contingent labor across the university and (gasp) affecting tenured
faculty. But, why? Why would AI and its integration into higher
education stop there?</p>
<p>This is cannon shot epistemology. Schroeder (and the <strong>many</strong> people who
produce similar self-serving arguments) relies on AI staying exactly
within the boundaries he assigns it in the same article that he admits
that the technology is outpacing us. To be clear, I am not arguing that
one specific outcome <strong>will</strong> occur. Rather, I am just asking us all to
admit that AI and its integration in our lives will develop in
unpredictable ways. With this in mind, maybe we shouldn&rsquo;t normalize the
possibility educators losing their jobs to it. As a thought.</p>

    </div>
  </article>

  

  

  
  

<div class="single-pagination">
    <hr />

    <div class="flexnowrap">

        <div class="single-pagination-prev">
            
            <div class="single-pagination-container-prev">
                <div class="single-pagination-text">←</div>
                <div class="single-pagination-text">
                    <a href="/posts/what-to-do-about-ai/">
                        I Don&#39;t Know What to Do About AI, and Neither Do You
                    </a>
                </div>
            </div>
            
        </div>

        <div class="single-pagination-next">
            
            <div class="single-pagination-container-next">
                <div class="single-pagination-text">
                    <a href="/posts/ignorant-schoolmaster/">
                        The Ignorant Schoolmaster: Five Lessons in Intellectual Emancipation (A Review)
                    </a>
                </div>
                <div class="single-pagination-text">→</div>
            </div>
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


    </main>
  </div>

  
  





    




  <footer>
    

    
    





    




    
    
    
    <p>Rick Wysocki is an English professor and PhD in rhetoric and writing studies. He is passionate about technologies and their social effects, new media, and philosophy.</p>
    


  </footer>

  
</body>

<script src="/js/theme-switch.js"></script>
<script defer src="/js/copy-code.js"></script>
</html>
