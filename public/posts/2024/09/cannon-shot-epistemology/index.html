<!doctype html>
<html><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-3ZRK4KWSBR"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-3ZRK4KWSBR');
    </script>

    
    <script src="https://kit.fontawesome.com/68345f7fdb.js" crossorigin="anonymous"></script>

    
   <link rel="stylesheet" href="http://localhost:1313/style.css">

    
    
    <title>Rick Wysocki | Cannon Shot Epistemology</title>
    
    <meta name="author" content="Rick Wysocki">

    <meta name="description" content="Maybe we can&#39;t shouldn&#39;t base our thinking on what AI is now. Maybe we should think harder about what it could become.">


    <link rel="canonical" href="http://localhost:1313/posts/2024/09/cannon-shot-epistemology/">

    <meta property="og:url" content="http://localhost:1313/posts/2024/09/cannon-shot-epistemology/">
  <meta property="og:site_name" content="Rick Wysocki">
  <meta property="og:title" content="Cannon Shot Epistemology">
  <meta property="og:description" content="Maybe we can’t shouldn’t base our thinking on what AI is now. Maybe we should think harder about what it could become.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-09-25T09:21:00-05:00">
    <meta property="article:modified_time" content="2024-09-25T09:21:00-05:00">
    <meta property="article:tag" content="Artificial Intelligence">
    <meta property="og:image" content="http://localhost:1313/posts/2024/09/cannon-shot-epistemology/featured.jpg">




    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&family=Karla:ital,wght@0,200..800;1,200..800&display=swap" rel="stylesheet">

</head>
<body class="p-4"><nav class="prose mt-4 text-xl max-w-4xl mx-auto mt-8 font-serif">
  <ul class="hidden md:flex flex-row pl-0 justify-start items-center gap-4 mt-0">

    
    <li class="list-none"><a href="/"><img class= "my-4 mx-auto h-7 rounded-full" src="/images/ghost.png" /></a></li>


    

    <li class="list-none "><a href="/" class="no-underline group transition duration-300">

      
      Home
    <span class="block max-w-0 group-hover:max-w-full transition-all duration-500 h-0.5 bg-indigo-600"></span>
    </a></li>
    

    <li class="list-none "><a href="/posts/" class="no-underline group transition duration-300">

      
      Posts
    <span class="block max-w-0 group-hover:max-w-full transition-all duration-500 h-0.5 bg-indigo-600"></span>
    </a></li>
    

    <li class="list-none "><a href="/portfolio/" class="no-underline group transition duration-300">

      
      Portfolio
    <span class="block max-w-0 group-hover:max-w-full transition-all duration-500 h-0.5 bg-indigo-600"></span>
    </a></li>
    

    <li class="list-none "><a href="/tags/" class="no-underline group transition duration-300">

      
      Tags
    <span class="block max-w-0 group-hover:max-w-full transition-all duration-500 h-0.5 bg-indigo-600"></span>
    </a></li>
    


  </ul>

  
  <div class="md:hidden flex justify-between">
  	<button class="outline-none mobile-menu-button">
  		<svg
  			class="w-6 h-6 text-gray-500"
  			x-show="!showMenu"
  			fill="none"
  			stroke-linecap="round"
  			stroke-linejoin="round"
  			stroke-width="2"
  			viewBox="0 0 24 24"
  			stroke="currentColor"
  		>
  		<path d="M4 6h16M4 12h16M4 18h16"></path>
  		</svg>
  	</button>
    <div>
    <li class="list-none"><a href="/"><img class= "my-4 mx-auto h-7 rounded-full" src="/images/ghost.png" /></a></li>
    </div>
  </div>

  
<div class="hidden mobile-menu">
	<ul class="py-6">
    

		<li class="list-none"><a href="/" class="block text-md py-1 underline decoration-indigo-500">
      
      Home
    </a></li>

    

		<li class="list-none"><a href="/posts/" class="block text-md py-1 underline decoration-indigo-500">
      
      Posts
    </a></li>

    

		<li class="list-none"><a href="/portfolio/" class="block text-md py-1 underline decoration-indigo-500">
      
      Portfolio
    </a></li>

    

		<li class="list-none"><a href="/tags/" class="block text-md py-1 underline decoration-indigo-500">
      
      Tags
    </a></li>

    
</ul>
</div>

<script>
	
	const btn = document.querySelector("button.mobile-menu-button");
	const menu = document.querySelector(".mobile-menu");

	
	btn.addEventListener("click", () => {
	menu.classList.toggle("hidden");
	});
</script>
</nav>



  <div class="mx-auto">

    
<div class="prose font-serif text-xl prose-a:text-indigo-600 leading-7 mx-auto mt-8">


<h1>Cannon Shot Epistemology</h1>




<img
    src="/posts/2024/09/cannon-shot-epistemology/featured_hu5459c0360c2b0cb7a147d2df0eb350ca_4230995_1000x0_resize_q75_box.jpg"
    alt="A cannon."
/>






  
  <div class="flex flex-row gap-4 items-center">
    <i class="fa fa-calendar"></i>
    <p class="my-0 text-gray-600">September 25, 2024</p>
  </div>
  
  
  
<div class="flex flex-row gap-4 items-center">
  <i class="fa fa-tag"></i>
  
  
    <a class="my-0 text-gray-600" href="/tags/artificial-intelligence">artificial intelligence</a>
  
</div>


<p>Shockingly (to myself), I&rsquo;ve fallen into a research project over the
past few months. In the process, I learned a small fact that I just
can&rsquo;t stop thinking about, and it connects to my dissatisfaction about
the takes I&rsquo;ve seen response to AI from educators. The connection is
oblique, but bear with me.</p>
<p>In earlier versions of the Western interstate system, the seas were
managed by the freedom of the seas doctrine, which tried to balance
nations&rsquo; coastal sovereignty against the use of the oceans for trade. In
the 18th century, this doctrine was upheld by a principle referred to as
the cannon-shot rule, where the territory of a state could be said to
extend only one coastal cannon-shot into its adjacent oceans. This crude
technique provided an available technical measurement for &ldquo;slicing&rdquo;
Earth within the freedom-of-the-seas doctrine that was held at the time.</p>
<p>A fascinatingly human detail is that it was apparently not considered that
cannons would eventually be made to shoot further, until they could.
As Tirza Meyer writes:</p>
<blockquote>
<p>As technology evolved &ndash;including even the simple fact that cannon
range increased &ndash; the necessity to establish exactly how far
territorial waters reached had become a pressing legal issue by the
early twentieth century. (43)</p>
</blockquote>
<p>The cannon-shot rule as an interstate agreement, then, required a fictive
reliability, a sense that:</p>
<ol>
<li>a technology would <em>not</em> progress, and</li>
<li>that it could be thoroughly and reliably understood.</li>
</ol>
<figure><img src="/posts/2024/09/cannon-shot-epistemology/virilio.jpg"
    alt="An image of Paul Virilio."><figcaption>
      <p>Paul Virilio.</p>
    </figcaption>
</figure>

<p>The story relates to Paul Virilio&rsquo;s theory of <em>the accident.</em> The
accident here refers to the inherent unreliability of technological
progress, the reality that technical development can only be managed up
to a point. As Virilio explained in a <a href="https://v2.nl/articles/surfing-the-accident" target="_blank" rel="nofollow noopener noreferrer">1998 interview</a>:</p>
<blockquote>
<p>You cannot separate the accident from reality. The accident is merely
the other face of substance, and Aristotle defined it already as such.
According to Aristotle, reality is a mixture of &ldquo;substans&rdquo; (i.e. what
is well established, from the Latin &ldquo;substare&rdquo;), and of &ldquo;accidens&rdquo;
(what &ldquo;falls into,&rdquo; from &ldquo;accidere&rdquo;). He characterized &ldquo;substans&rdquo; as
absolute and necessary, and &ldquo;accidens&rdquo; as relative and fortuitous.
Consequently, reality is made up of these two dimensions. As soon as
something is well established (a substance), it is necessarily
accompanied by something unreliable, which can trigger off forces
difficult to contain at any moment. Technology can only progress in a
struggle against the accident.</p>
</blockquote>
<p>For Virilio, these technological accidents used to be more localized. But,
given the interconnected nature of the modern world-system, technical
accidents have become ever more &ldquo;integral,&rdquo; that is,
globally-implicating.</p>
<p>What&rsquo;s interesting to me, here, is that the technical accident relies on
a limited knowledge about the future and an inability to predict
technological development. Moreover, the questions we <em>do</em> ask about
technological development <a href="https://www.ben-evans.com/benedictevans/2017/01/11/wrongquestions" target="_blank" rel="nofollow noopener noreferrer">are often the wrong ones</a>, because we can&rsquo;t
extrapolate present conditions into the future. This was clearly the
case for the establishment of the cannon-shot rule. It offered an
available measurement but couldn&rsquo;t predict its integral accident, the
unknown technical progress of the cannon and its effects on legal
interstate relations.</p>
<p>This inability to reckon with and ultimately face <em>unknowingness</em> is, in
my opinion, <em>the central</em> <em>problem</em> of current conversations about AI in
higher education. We simply don&rsquo;t have a roadmap here. Some would like
to travel the well-worn territory of how we responded to earlier
technologies, which is endearingly human but wrong. Others want to not
engage with the issue at all.</p>
<p>But more and more I feel like higher ed isn&rsquo;t asking the right
questions, either about generative AI&rsquo;s substance or its possible
accidents. Environmental concerns are certainly an obvious integral
accident, not only of AI but of networked technologies in general. But
what other accidents can we try (to the extent that can try) to imagine?</p>
<p>It&rsquo;s worth noting that Virilio was explicitly considering the
possibility of AI in thinking through these questions. In the same
interview, he says:</p>
<blockquote>
<p>Well, it is true that the fifth generation computers will not only be
able to learn but also to bring forth other computers. What bothers me
most in this idea of self-learning computers is the closed circuit
character of these systems. The world of computing generally is
plagued by this closed loop problem, which is what makes it so
dangerous in the hands of a totalitarian system. In order to avoid
this &ldquo;Gleichschaltung,&rdquo; as the Nazis called it, it is necessary to
structure new computer systems as open systems.</p>
</blockquote>
<p>Even this comment, from 1998, suggests something that academic
institutions are, predictably, ignoring: the proprietary and closed
nature of the AI systems that we will seemingly very likely be building, in one way or another,
into the foundations of learning in years to come. This may not pose a
clear problem now, but there is always an accident around the corner.</p>
<p>Are we treating AI as the evolving technology that it is? Or are we
spending our time building &ldquo;cannon shot rules&rdquo; around ultimately
unstable and undetermined assumptions? I&rsquo;m starting to think about
&ldquo;cannon-shot epistemology&rdquo; to name the ways we build knowledge and infrastructure on an
human but incorrect assumption of stable technology.</p>
<p>So, what does this look like? Here&rsquo;s an example. When folks in higher ed conversations <em>do</em> try to imagine the future of AI, they demonstrate cannon shot epistemology in observably specific ways.</p>
<p>A <a href="https://www.insidehighered.com/opinion/blogs/online-trending-now/2024/09/25/near-future-vision-ai-higher-ed" target="_blank" rel="nofollow noopener noreferrer">recent opinion piece by Ray Schroeder</a> published by <em>Inside Higher Ed</em> demonstrates them. It starts well enough by acknowledging that AI is outpacing higher education but that we still need to find ways to imagine its future in order to respond to it effectively. Good. I agree. This, so far, is exactly right.</p>
<p>But where does Schroeder&rsquo;s imagination take him?</p>
<blockquote>
<p>I see us replacing midlevel administrators with intelligent agents that can efficiently and effectively make decisions that are thoroughly documented and adaptive to changing goals and outcomes.</p>
</blockquote>
<p>One paragraph later:</p>
<blockquote>
<p>Startling as it may seem to some, I can see these advanced models, such as those with Ph.D. reasoning, filling adjunct faculty posts while overseen by human professors. The long-running OpenAI-funded Khanmigo project demonstrates that key teaching, tutoring and personalization skills can be delivered by generative AI.</p>
</blockquote>
<p>Putting aside the obvious and offensive groundwork being laid <strong>against</strong> non-tenure faculty and administrative staff, <em>why is it that everyone&rsquo;s imagination seems to exhaust itself just before AI could possibly affect <strong>them?!</strong></em> Elsewhere in this exact same article, Schroeder discusses how &ldquo;Multiple societies have been formed and fascinating communities have been built by intelligent agents.&rdquo; He&rsquo;s talking about Minecraft, which is embarrassing, but the point is this: he can imagine AI creating societies, but he can&rsquo;t imagine AI developing one half-inch past firing contingent labor across the university and (gasp) affecting tenured faculty. But, why? Why would AI and its integration into higher education stop there?</p>
<p>This is cannon shot epistemology. Schroeder (and the <strong>many</strong> people who produce similar self-serving arguments) relies on AI staying exactly within the boundaries he assigns it in the same article that he admits that the technology is outpacing us. To be clear, I am not arguing that one specific outcome <strong>will</strong> occur. Rather, I am just asking us all to admit that AI and its integration in our lives will develop in unpredictable ways. With this in mind, maybe we shouldn&rsquo;t normalize the possibility educators losing their jobs to it. As a thought.</p>


</div>



    
    <div id="disqus_thread" class="max-w-4xl mx-auto mt-8 px-8"></div>
<script>
    

    

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://rickwysocki.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<footer class="mx-auto shadow-lg max-w-lg p-6 text-center sm:py-4 sm:flex sm:items-center sm:space-y-0 sm:space-x-6 mt-16 mb-4">
    
    <img class= "sm:mx-0 sm:shrink-0 mx-auto h-24 rounded-full" src="/images/bio-photo.jpg" />
    <div class="text-left">
      <h6 class="font-bold text-indigo-600">Rick Wysocki</h6>
      <p class="text-gray-600 font-serif">Writing Expert | Educator | Technical Communicator</p>
      <p class="font-serif">I am a professor at Ball State University. I&#39;m passionate about effective communication, research, and information design. My free time is spent practicing Zen, reading, learning new technologies, and lifting weights.</p>
      <div class="flex gap-4 mt-2 text-xl text-indigo-600">
        <a href="mailto:rlwysockijr@gmail.com"><i class="fa fa-envelope"></i></a>
        <a href="https://github.com/rickwysocki"><i class="fa fa-github"></i></a>
        
        <a href="https://mas.to/@rickwysocki"><i class="fab fa-mastodon"></i></a>


    </div>
    </div>

</footer>
</div>

</body>
</html>
