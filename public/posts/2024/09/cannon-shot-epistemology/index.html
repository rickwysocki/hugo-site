<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
      <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Cannon Shot Epistemology | Rick Wysocki</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <link rel="stylesheet" href="styles.css">
    
    <link rel="stylesheet" href="/css/main.css">

    
      <script src="/js/main.js"></script>


</head>
<body>
  <header>
    

  <nav>
    <ul>
    <li>
      <a href="/">Home</a>
    </li>
    <li>
      <a href="/posts/">Posts</a>
    </li>
    <li>
      <a href="/portfolio/">Portfolio</a>
    </li>
    <li>
      <a href="/tags/">Tags</a>
    </li>
    </ul>
  </nav>


 <div class="container my-5">
   <h1>Rick Wysocki</h1>
   
      <div class="col-lg-8 px-0">
        <p>You've successfully loaded up the Bootstrap starter example. It includes <a href="https://getbootstrap.com/">Bootstrap 5</a> via the <a href="https://www.jsdelivr.com/package/npm/bootstrap">jsDelivr CDN</a> and includes an additional CSS and JS file for your own code.</p>
        <p>Feel free to download or copy-and-paste any parts of this example.</p>

        <hr class="col-1 my-4">


      </div>
      </div>
      

  </header>
  <main class="container">
    
  <h1>Cannon Shot Epistemology</h1>

  
  
  <time datetime="2024-09-25T09:21:00-05:00">September 25, 2024</time>

  <p>Shockingly (to myself), I&rsquo;ve fallen into a research project over the
past few months. In the process, I learned a small fact that I just
can&rsquo;t stop thinking about, and it connects to my dissatisfaction about
the takes I&rsquo;ve seen response to AI from educators. The connection is
oblique, but bear with me.</p>
<p>In earlier versions of the Western interstate system, the seas were
managed by the freedom of the seas doctrine, which tried to balance
nations&rsquo; coastal sovereignty against the use of the oceans for trade. In
the 18th century, this doctrine was upheld by a principle referred to as
the cannon-shot rule, where the territory of a state could be said to
extend only one coastal cannon-shot into its adjacent oceans. This crude
technique provided an available technical measurement for &ldquo;slicing&rdquo;
Earth within the freedom-of-the-seas doctrine that was held at the time.</p>
<p>A fascinatingly human detail is that it was apparently not considered that
cannons would eventually be made to shoot further, until they could.
As Tirza Meyer writes:</p>
<blockquote>
<p>As technology evolved &ndash;including even the simple fact that cannon
range increased &ndash; the necessity to establish exactly how far
territorial waters reached had become a pressing legal issue by the
early twentieth century. (43)</p>
</blockquote>
<p>The cannon-shot rule as an interstate agreement, then, required a fictive
reliability, a sense that:</p>
<ol>
<li>a technology would <em>not</em> progress, and</li>
<li>that it could be thoroughly and reliably understood.</li>
</ol>
<figure><img src="/posts/2024/09/cannon-shot-epistemology/virilio.jpg"
    alt="An image of Paul Virilio."><figcaption>
      <p>Paul Virilio.</p>
    </figcaption>
</figure>

<p>The story relates to Paul Virilio&rsquo;s theory of <em>the accident.</em> The
accident here refers to the inherent unreliability of technological
progress, the reality that technical development can only be managed up
to a point. As Virilio explained in a <a href="https://v2.nl/articles/surfing-the-accident">1998 interview</a>:</p>
<blockquote>
<p>You cannot separate the accident from reality. The accident is merely
the other face of substance, and Aristotle defined it already as such.
According to Aristotle, reality is a mixture of &ldquo;substans&rdquo; (i.e. what
is well established, from the Latin &ldquo;substare&rdquo;), and of &ldquo;accidens&rdquo;
(what &ldquo;falls into,&rdquo; from &ldquo;accidere&rdquo;). He characterized &ldquo;substans&rdquo; as
absolute and necessary, and &ldquo;accidens&rdquo; as relative and fortuitous.
Consequently, reality is made up of these two dimensions. As soon as
something is well established (a substance), it is necessarily
accompanied by something unreliable, which can trigger off forces
difficult to contain at any moment. Technology can only progress in a
struggle against the accident.</p>
</blockquote>
<p>For Virilio, these technological accidents used to be more localized. But,
given the interconnected nature of the modern world-system, technical
accidents have become ever more &ldquo;integral,&rdquo; that is,
globally-implicating.</p>
<p>What&rsquo;s interesting to me, here, is that the technical accident relies on
a limited knowledge about the future and an inability to predict
technological development. Moreover, the questions we <em>do</em> ask about
technological development <a href="https://www.ben-evans.com/benedictevans/2017/01/11/wrongquestions">are often the wrong ones</a>, because we can&rsquo;t
extrapolate present conditions into the future. This was clearly the
case for the establishment of the cannon-shot rule. It offered an
available measurement but couldn&rsquo;t predict its integral accident, the
unknown technical progress of the cannon and its effects on legal
interstate relations.</p>
<p>This inability to reckon with and ultimately face <em>unknowingness</em> is, in
my opinion, <em>the central</em> <em>problem</em> of current conversations about AI in
higher education. We simply don&rsquo;t have a roadmap here. Some would like
to travel the well-worn territory of how we responded to earlier
technologies, which is endearingly human but wrong. Others want to not
engage with the issue at all.</p>
<p>But more and more I feel like higher ed isn&rsquo;t asking the right
questions, either about generative AI&rsquo;s substance or its possible
accidents. Environmental concerns are certainly an obvious integral
accident, not only of AI but of networked technologies in general. But
what other accidents can we try (to the extent that can try) to imagine?</p>
<p>It&rsquo;s worth noting that Virilio was explicitly considering the
possibility of AI in thinking through these questions. In the same
interview, he says:</p>
<blockquote>
<p>Well, it is true that the fifth generation computers will not only be
able to learn but also to bring forth other computers. What bothers me
most in this idea of self-learning computers is the closed circuit
character of these systems. The world of computing generally is
plagued by this closed loop problem, which is what makes it so
dangerous in the hands of a totalitarian system. In order to avoid
this &ldquo;Gleichschaltung,&rdquo; as the Nazis called it, it is necessary to
structure new computer systems as open systems.</p>
</blockquote>
<p>Even this comment, from 1998, suggests something that academic
institutions are, predictably, ignoring: the proprietary and closed
nature of the AI systems that we will seemingly very likely be building, in one way or another,
into the foundations of learning in years to come. This may not pose a
clear problem now, but there is always an accident around the corner.</p>
<p>Are we treating AI as the evolving technology that it is? Or are we
spending our time building &ldquo;cannon shot rules&rdquo; around ultimately
unstable and undetermined assumptions? I&rsquo;m starting to think about
&ldquo;cannon-shot epistemology&rdquo; to name the ways we build knowledge and infrastructure on an
human but incorrect assumption of stable technology.</p>
<p>So, what does this look like? Here&rsquo;s an example. When folks in higher ed conversations <em>do</em> try to imagine the future of AI, they demonstrate cannon shot epistemology in observably specific ways.</p>
<p>A <a href="https://www.insidehighered.com/opinion/blogs/online-trending-now/2024/09/25/near-future-vision-ai-higher-ed">recent opinion piece by Ray Schroeder</a> published by <em>Inside Higher Ed</em> demonstrates them. It starts well enough by acknowledging that AI is outpacing higher education but that we still need to find ways to imagine its future in order to respond to it effectively. Good. I agree. This, so far, is exactly right.</p>
<p>But where does Schroeder&rsquo;s imagination take him?</p>
<blockquote>
<p>I see us replacing midlevel administrators with intelligent agents that can efficiently and effectively make decisions that are thoroughly documented and adaptive to changing goals and outcomes.</p>
</blockquote>
<p>One paragraph later:</p>
<blockquote>
<p>Startling as it may seem to some, I can see these advanced models, such as those with Ph.D. reasoning, filling adjunct faculty posts while overseen by human professors. The long-running OpenAI-funded Khanmigo project demonstrates that key teaching, tutoring and personalization skills can be delivered by generative AI.</p>
</blockquote>
<p>Putting aside the obvious and offensive groundwork being laid <strong>against</strong> non-tenure faculty and administrative staff, <em>why is it that everyone&rsquo;s imagination seems to exhaust itself just before AI could possibly affect <strong>them?!</strong></em> Elsewhere in this exact same article, Schroeder discusses how &ldquo;Multiple societies have been formed and fascinating communities have been built by intelligent agents.&rdquo; He&rsquo;s talking about Minecraft, which is embarrassing, but the point is this: he can imagine AI creating societies, but he can&rsquo;t imagine AI developing one half-inch past firing contingent labor across the university and (gasp) affecting tenured faculty. But, why? Why would AI and its integration into higher education stop there?</p>
<p>This is cannon shot epistemology. Schroeder (and the <strong>many</strong> people who produce similar self-serving arguments) relies on AI staying exactly within the boundaries he assigns it in the same article that he admits that the technology is outpacing us. To be clear, I am not arguing that one specific outcome <strong>will</strong> occur. Rather, I am just asking us all to admit that AI and its integration in our lives will develop in unpredictable ways. With this in mind, maybe we shouldn&rsquo;t normalize the possibility educators losing their jobs to it. As a thought.</p>

  
  <div>
    <div>Tags:</div>
    <ul>
        <li><a href="/tags/artificial-intelligence/">Artificial Intelligence</a></li>
    </ul>
  </div>


  </main>
  <footer>
    <p>Copyright 2025. All rights reserved.</p>

  </footer>
</body>
</html>
